\section{Evaluation}\label{sec:evaluation}

% The source counts were generated by:
% $ cd benchmarks/haskell16/pos/
% $ find . -type f -name '*.hs' -exec sed -i '' s/\{-@/\{-#LH/ {} +
% $ sloccount

\begin{figure}[t!]
\begin{center}
\begin{tabular}{lllr}
\toprule
  \multicolumn{3}{l}{\textbf{CATEGORY}}              & \textbf{LOC} \\
\toprule
  \textbf{I.} & \multicolumn{3}{l}{\textbf{Arithmetic}} \\[0.05in]
   & Fibonacci      & \S~\ref{sec:overview}          &  48 \\ % Overview.hs
   & Ackermann      & \citep{ackermann}
                    , Fig.~\ref{fig:ackermann}       & 280 \\ % Ackermann.hs

  \midrule

  \textbf{II.} & \multicolumn{3}{l}{\textbf{Algebraic Data Types}} \\[0.05in]

  & Fold Universal & \citep{agdaequational}          & 105 \\ % FoldrUniversal.hs
  & Fold Fusion    & \citep{agdaequational}          &     \\

  \midrule

  \textbf{III.} & \textbf{Typeclasses} & Fig~\ref{fig:laws} & \\[0.05in]
  & Monoid         & \tPeano, \tMaybe, \tList        & 189 \\ % Monoid*.hs
  & Functor        & \tMaybe, \tList, \tId, \tReader & 296 \\ % Functor*.hs - FunctorReader.hs
  & Applicative    & \tMaybe, \tList, \tId, \tReader & 578 \\ % Applicative*.hs
  & Monad          & \tMaybe, \tList, \tId, \tReader & 435 \\ % Monad*.hs

  \midrule

  \textbf{IV.} & \multicolumn{3}{l}{\textbf{Functional Correctness}} \\[0.05in]
  & SAT Solver     & \citep{Zombie}                  & 133 \\ % Solver.hs
  & Unification    & \citep{Sjoberg2015}             & 200 \\ % Unification

  \midrule

  \textbf{V.} & \multicolumn{3}{l}{\textbf{Deterministic Parallelism}} \\[0.05in]
  & Concurrent Sets     & \S~\ref{sec:set}           & 906 \\ % VerifiedEq/Ord + PureSet.hs + SLSet.hs
  & $n$-body simulation & \S~\ref{sec:nbody}         & 930 \\ % VerifiedEq/Ord + Inj/Iso + allpairs.hs
  & Parallel Reducers   & \S~\ref{sec:reducer}       &  55 \\ % VerifiedSemigroup + Iso + IntegerSumReduction2.hs

  \midrule

  \multicolumn{3}{l}{\textbf{TOTAL}}                 & 4155 \\
\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Summary of Case Studies}}
\label{fig:eval-summary}
\end{figure}

We have implemented refinement reflection
in \toolname. 
%
In this section, we evaluate our approach
by using \toolname to verify a variety of
deep specifications of Haskell functions
drawn from the literature and categorized
in Figure~\ref{fig:eval-summary},
totalling about 4000 lines of specifications
and proofs.
%
Next, we detail each of the first four classes of
specifications, illustrate how they were
verified using refinement reflection, and
discuss the strengths and weaknesses of
our approach.
%
\emph{All} of these proofs require refinement
reflection, \ie are beyond the scope of shallow
refinement typing.

\mypara{Proof Strategies.}
%
Our proofs use three building blocks, that are seamlessly
connected via refinement typing:
%
\begin{itemize}
  \item \emphbf{Un/folding}
     definitions of a function @f@ at
     arguments @e1...en@, which due
     to refinement reflection, happens
     whenever the term @f e1 ... en@
     appears in a proof.
     For exposition, we render the function
     whose un/folding is relevant as @#f#@;

  \item \emphbf{Lemma Application}
     which is carried out by using
     the ``because'' combinator
     ($\because$) to instantiate
     some fact at some inputs;

  \item \emphbf{SMT Reasoning}
     in particular, \emph{arithmetic},
     \emph{ordering} and \emph{congruence closure}
     which kicks in automatically (and predictably!),
     allowing us to simplify proofs by not
     having to specify, \eg which subterms
     to rewrite.
\end{itemize}

\subsection{Arithmetic Properties} \label{subsec:arith} \label{subsec:ackermann}

The first category of theorems pertains to the textbook
Fibonacci and Ackermann functions.
%
The former were shown in \S~\ref{sec:overview}.
%
The latter are summarized in Figure~\ref{fig:ackermann},
which shows two alternative definitions for the
Ackermann function.
%
We proved equivalence of the definition (Prop 1)
and various arithmetic relations between
them (Prop 2 --- 13), by mechanizing the
proofs from~\cite{ackermann}.

\begin{figure}[t!]
\textbf{Ackermann's Function}
\[
\begin{array}{lr}
\ack{n}{x} \defeq
 \left\{
\begin{array}{l}
\setlength\arraycolsep{0pt}
\begin{array}{ll}
      x+2 &\quad \text{, if}\ n=0 \\
      2   &\quad \text{, if}\ x=0 \\
\end{array} \\
\ack{n-1}{\ack{n}{x-1}}
\end{array}
\right.
&
\iack{h}{n}{x} \defeq
 \left\{
\begin{array}{l}
      x \quad \text{, if}\ h=0 \\
      \ack{n}{\iack{h-1}{n}{x}}
\end{array}
\right.
\end{array}
 \]

\textbf{Properties}
$$
\begin{array}{lrcrcl}
1.&                &&            \ack{n+1}{x}   &=& \iack{x}{n}{2}\\
2.&                &&            x + 1          &<& \ack{n}{x}\\
3.&                &&            \ack{n}{x}     &<& \ack{n}{x+1}\\
4.& x < y          &\Rightarrow& \ack{n}{x}     &<& \ack{n}{y}\\
5.& 0 < x          &\Rightarrow& \ack{n}{x}     &<& \ack{n+1}{x}\\
6.& 0 < x, n < m   &\Rightarrow& \ack{n}{x}     &<& \ack{m}{x}\\
7.&                &&            \iack{h}{n}{x} &<& \iack{h+1}{n}{x}\\
8.&                &&            \iack{h}{n}{x} &<& \iack{h}{n}{x+1}\\
9.& x<y            &\Rightarrow& \iack{h}{n}{x} &<& \iack{h}{n}{y}\\
10.&               &\Rightarrow& \iack{h}{n}{x} &<& \iack{h}{n+1}{x}\\
11.& 0<n, l-2 < x  &\Rightarrow& x + l          &<& \ack{n}{x}\\
12.& 0<n, l-2 < x  &\Rightarrow& \iack{l}{n}{x} &<& \ack{n+1}{x}\\
13.&               &&            \iack{x}{n}{y} &<& \ack{n+1}{x+y}\\
\end{array}
$$
\caption{\textbf{Ackermann Properties~\citep{ackermann},
$\forall n, m, x, y, h, l \geq 0$}}
\label{fig:ackermann}
\end{figure}

\mypara{Monotonicity}
%
Prop 3. shows that \ack{n}{x} is increasing on $x$.
%
We derived Prop 4. by applying @fMono@ theorem
from \S~\ref{sec:examples} with input function
the partially applied Ackermann Function
$\ack{n}{\star}$.
%
Similarly, we derived the monotonicity Prop 9. by
applying @fMono@ to the locally increasing Prop. 8
and $\iack{h}{n}{\star}$.
%
Prop 5. proves that \ack{n}{x} is increasing
on the \emph{first} argument $n$.
%
As @fMono@ applies to the \emph{last} argument
of a function, we cannot directly use it to
derive Prop 6.
%
Instead, we define a variant @fMono2@ that works
on the first argument of a binary function, and
use it to derive Prop 6.

%%\mypara{Existentials}
%%%
%%Properties 11. and 12. are described in~\citep{ackermann}
%%to hold for almost every $x$.
%%%
%%That is, Property 11. is described as
%%$\exists x_0. x_0 < x \Rightarrow x + l < \ack{n}{x}$.
%%%
%%Refinement types cannot express existentials,
%%thus expressing the above property is (currently)
%%not feasible, but the minimum $x$ was easy to retrieve.
%%%
%%Thus, we expressed the above statement by specifying $x_0 = l - 2$.
%%%
%%\NV{``Almost'' means in English for large x.
%%In math it translates to there exist some x0 such that...
%%in LiquidHaskell, and due to lack of existentials
%%I had to find (``retrieve'') an x0 = l-2 above which
%%the property holds For almost see big o notation, where
%%there exists a c such that...to specify these properties
%%in LH you need to give a concrete c But, this paragraph
%%summarizes a lot of internal thinking, so feel free to
%%rephrase. I wanted it in, because when I saw the lemma
%%stating this property holds almost everywhere I though
%%we could not express it, but we can!}

%
\mypara{Constructive Proofs}
%
In \citep{ackermann} Prop 12. was proved by constructing
an auxiliary \emph{ladder} that counts the number of
(recursive) invocations of the Ackermann function, and
uses this count to bound \iack{h}{n}{x} and \ack{n}{x}.
%
It turned out to be straightforward and natural
to formalize the proof just by defining the
@ladder@ function in Haskell, reflecting it,
and using it to formalize the algebra from~\citep{ackermann}.

\subsection{Algebraic Data Properties}
\label{subsec:fold}

The second category of properties pertain to
algebraic data types.
%, \eg \emph{folding} over lists.

\mypara{Fold Univerality}
%
Next, we proved properties of list folding, such as
the following, describing the \emph{universal}
property of right-folds~\citep{agdaequational}:
%
\begin{code}
foldr_univ
  :: f:(a -> b -> b)
  -> h:([a] -> b)
  -> e:b
  -> ys:[a]
  -> base:{h [] = e }
  -> stp:(x:a ->l:[a]->{h(x:l) = f x (h l)})
  -> {h ys = foldr f e ys}
\end{code}
%
Our proof @foldr_univ@ differs from the one in Agda,
in two ways.
%
First, we encode Agda's universal quantification over
@x@ and @l@ in the assumption @stp@ using a function type.
%
Second, unlike Agda, \toolname
does not support implicit arguments,
so at \emph{uses} of @foldr_univ@
the programmer must explicitly
provide arguments for @base@
and @stp@, as illustrated below.

\mypara{Fold Fusion}
%
Let us define the usual composition operator:
%
\begin{code}
reflect . :: (b -> c) -> (a -> b) -> a -> c
f . g     = \x -> f (g x)
\end{code}
%
We can prove the following @foldr_fusion@ theorem
(that shows operations can be pushed inside a @foldr@),
by applying @foldr_univ@ to explicit @bas@ and @stp@ proofs:
%
\begin{code}
  foldr_fusion
   :: h:(b -> c)
   -> f:(a -> b -> b)
   -> g:(a -> c -> c)
   -> e:b -> z:[a] -> x:a -> y:b
   -> fuse: {h (f x y) = g x (h y)})
   -> {(h . foldr f e) z = foldr g (h e) z}

  foldr_fusion h f g e ys fuse
    = foldr_univ g (h . foldr f e) (h e) ys
        (fuse_base h f e)
        (fuse_step h f e g fuse)
\end{code}
%
where @fuse_base@ and @fuse_step@ prove the
base and inductive cases, and for example
@fuse_base@ is a function with type
%
\begin{code}
fuse_base :: h:(b->c) -> f:(a->b->b) -> e:b
          -> {(h . foldr f e) [] = h e}
\end{code}

\subsection{Typeclass Laws}

\begin{figure}[t!]
\begin{center}
\begin{tabular}{rl}

\toprule

\multicolumn{2}{c}{\textbf{Monoid}} \\
{Left Ident.}  & $\emempty\ x\ \emappend\  \equiv x$  \\
{Right Ident.} & $x\ \emappend\ \emempty \equiv x$  \\
{Associativity}  & $(x\ \emappend\ y)\ \emappend\ z \equiv x\ \emappend\ (y\ \emappend\ z)$ \\

\midrule

\multicolumn{2}{c}{\textbf{Functor}} \\
{Ident.}     & $\efmap\ \eid\ xs \equiv \eid\ xs$ \\
{Distribution} & $\efmap\ (g\ecompose\ h)\ xs \equiv (\efmap\ g\ \ecompose\ \efmap\ h)\ xs$\\

\midrule

\multicolumn{2}{c}{\textbf{Applicative}} \\

{Ident.}      & $\epure \eid \eseq\ v \equiv v$ \\
{Compos.}     & $\epure (\ecompose) \eseq u \eseq v \eseq w \equiv u \eseq (v \eseq w)$ \\
{Homomorph.}  & $\epure\ f\ \eseq\ \epure\ x \equiv \epure\ (f\ x)$\\
{Interchange} & $u\ \eseq\ \epure\ y \equiv \epure\ (\$\ y) \ \eseq \ u$ \\

\midrule
\multicolumn{2}{c}{\textbf{Monad}} \\
{Left Ident.}   & $\ereturn\ a \ebind f \equiv f\ a$ \\
{Right Ident.}  & $m \ebind \ereturn \equiv m$ \\
{Associativity} & $(m\ebind f) \ebind g \equiv m\ebind (\lambda x \rightarrow f\ x \ebind g)$\\
\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Typeclass Laws verified using \toolname}}
\label{fig:laws}
\end{figure}
We used \toolname to prove the Monoid, Functor,
Applicative and Monad Laws, summarized in
Figure~\ref{fig:laws}, for various user-defined
instances summarized in Figure~\ref{fig:eval-summary}.

%% The purpose of these proofs is to investigate the
%% proving abilities of \libname.
%% For this purpose, we defined the appropriate class
%% operators on user defined lists, instead of using
%% Haskell's predefined class instances.
%% %
%% In the near future, we plan to embed these proofs
%% to check the laws on real Haskell instances,
%% but this requires some engineering from the
%% \liquidHaskell team.

\mypara{Monoid Laws}
%
A Monoid is a datatype equipped with an associative
binary operator $\emappend$ and an \emph{identity}
element $\emempty$.
%
We use \toolname to prove that
%
@Peano@ (with @add@ and @Z@),
@Maybe@ (with a suitable @mappend@ and @Nothing@), and
@List@ (with append @++@ and @[]@) satisfy the monoid laws.
%
For example, we prove that @++@ (\S~\ref{subsec:list})
is associative by reifying the textbook proof~\cite{HuttonBook}
into a Haskell function, where the induction
corresponds to case-splitting and recurring
on the first argument:
%
\begin{mcode}
assoc :: xs:[a] -> ys:[a] -> zs:[a] ->
       {(xs ++ ys) ++ zs = xs ++ (ys ++ zs)}

assoc [] ys zs     = ([] #++# ys) ++ zs
                   =. [] #++# (ys ++ zs)
                   ** QED
assoc (x:xs) ys zs = ((x:  xs)#++# ys) ++ zs
                   =. (x: (xs ++ ys))#++# zs
                   =.  x:((xs ++ ys) ++ zs)
                   =.  x: (xs ++ (ys ++ zs))
                       $\because$ assoc xs ys zs
                   =. (x:xs)  #++# (ys ++ zs)
                   ** QED
\end{mcode}


\mypara{Functor Laws}
%
A type is a functor if it has a function
@fmap@ that satisfies the \emph{identity}
and \emph{distribution} (or fusion) laws
in Figure~\ref{fig:laws}.
%
For example, consider the proof of
the @fmap@ distribution law for the lists,
also known as ``map-fusion'', which is the
basis for important optimizations in
GHC~\cite{ghc-map-fusion}.
%
We reflect the definition of @fmap@:
%
\begin{code}
  reflect map :: (a -> b) -> [a] -> [b]
  map f []     = []
  map f (x:xs) = f x : fmap f xs
\end{code}
%
and then specify fusion and verify it by an inductive proof:
% by induction (recursion) on the list argument:
%
\begin{mcode}
  map_fusion
    :: f:(b -> c) -> g:(a -> b) -> xs:[a]
    -> {map (f . g) xs = (map f . map g) xs}
\end{mcode}

%%\begin{mcode}
%%  map_fusion f g []
%%    =  ((map f) #.# (map g)) []
%%    =. (map f) (#map# g [])
%%    =. #map# f []
%%    =. []
%%    =. #map# (f . g) []
%%    ** QED
%%
%%  map_fusion f g (x:xs)
%%    =   #map# (f . g) (x:xs)
%%    =. (f . g) x : map (f . g) xs
%%    =. (f . g) x : (map f #.# map g) xs
%%       $\because$  map_fusion f g xs
%%    =. (f# . #g) x : map f (map g xs)
%%    =. f   (g  x): map f (map g xs)
%%    =. #map# f (g x: map g xs)
%%    =. map f   (#map# g (x:xs))
%%    =. (map f #.# map g)(x:xs)
%%    ** QED
%%\end{mcode}
%%
% \NV{Say why we need defunctionalization}
% \NV{We use app function like HALO (link to the theory)}

% \mypara{Applicative}

\mypara{Monad Laws}
%
The monad laws, which relate the
properties of the two operators
$\ebind$ and $\ereturn$ (Figure~\ref{fig:laws}),
refer to $\lambda$-functions,
thus their proof exercises
our support for defunctionalization
and $\eta$- and $\beta$-equivalence.
% and the extensionality axioms to prove.
%
For example, consider the proof of the
associativity law for the list monad.
First, we reflect the bind operator:
%
\begin{code}
  reflect (>>=) :: [a] -> (a -> [b]) -> [b]
  (x:xs) >>= f = f x ++ (xs >>= f)
  []     >>= f = []
\end{code}
%
Next, we define an abbreviation for the associativity property:
%
\begin{code}
type AssocLaw m f g =
  {m >>= f >>= g = m >>= (\x -> f x >>= g)}
\end{code}
%
Finally, we can prove that the list-bind is associative:
%
\begin{mcode}
assoc :: m:[a] -> f:(a ->[b]) -> g:(b ->[c])
      -> AssocLaw m f g
assoc [] f g
  =  [] #>>=# f >>= g
  =. [] #>>=# g
  =. []
  =. [] #>>=# (\x -> f x >>= g) ** QED
assoc (x:xs) f g
  =  (x:xs) #>>=# f  >>= g
  =. (f x ++ xs >>= f) >>= g
  =. (f x >>= g) ++ (xs >>= f >>= g)
     $\because$ bind_append (f x) (xs >>= f) g
  =. (f x >>= g) ++ (xs >>= \y -> f y >>= g)
     $\because$ assoc xs f g
  =. (\y -> f y >>= g) x ++
     (xs >>= \y -> f y >>= g)
     $\because$ $\beta$eq f g x
  =. (x:xs) #>>=# (\y -> f y >>= g) ** QED
\end{mcode}
%
Where the bind-append fusion lemma states that:
%
\begin{code}
bind_append ::
  xs:[a] -> ys:[a] -> f:(a -> [b]) ->
  {(xs++ys) >>= f = (xs >>= f)++(ys >>= f)}
\end{code}
%
Notice that the last step requires
$\beta$-equivalence on anonymous
functions, which we get by explicitly
inserting the redex in the logic,
via the following lemma with @trivial@ proof
%
\begin{mcode}
  $\beta$eq :: f:_ -> g:_ -> x:_ ->
     {bind (f x) g = (\y -> bind (f y) g) x}
  $\beta$eq _ _ _ = trivial
\end{mcode}
%
% \RJ{TODO:discuss $\alpha$ and $\beta$ equality, axiom text commented out}

%%\NV{This is new, text can be simplified}
%%\mypara{The Reader Monad} was the most challenging of our benchmarks,
%%as the @Reader r a@ data type wraps the value @a@ inside a reader-only state,
%%represented by a lambda argument @r@
%%%
%%\begin{mcode}
%%  data Reader r a = R { runR :: r -> a }
%%\end{mcode}
%%%
%%Because of the functional structure of the Reader data type,
%%equational proofs proceed via unfolding wrapped inside the abstracted
%%state, thus proving equalities made heavy use of the extensionality
%%function equality~\ref{subsec:extensionality} requiring
%%usage $\eta$-reduced proof arguments.
%%%
%%As an example, proof of monadic associativity performs an
%%unfolding if the reflected bind wrapped inside \textit{two} lambda arguments:
%%%
%%\begin{code}
%%     R (\r2 -> runR ((\r4 ->
%%      R (\r3 ->
%%       runR (g ((runR (f r4)) r3)) r3)
%%     ) (x r2)) r2)
%%  =. R (\r2 -> runR ((\r4 ->
%%      f r4 #>>=# g
%%     ) (x r2)) r2)
%%\end{code}
%%%
%%Even though the intermediate equation step seemed
%%straightforward, it required usage of extensionality equality twice,
%%thus two intermediate $\eta$-expanded helper proofs
%%
%%% To prove this equality, in the logic,
%%% the anonymous functions are represented
%%% as functional variables axiomatized with
%%% extensionality axioms.
%%% %
%%% Thus, in the logic, we define @f'@ and
%%% the axioms @forall x. f' x = f x >>= g@
%%% and @forall g x. (f' x = g x) => f' = g@.
%%% %
%%% These two axioms are sufficient to prove
%%% 1. $\eta$-equivalence that is required in
   %%% the last step of the inductive case; and
%%% 2. $\beta$-equivalence that is required
   %%% to prove that our proof
   %%% @xs >>= f >>= g =. xs >>= (\y -> f y >>= g)@
   %%% implies the specification.


%% In all, most of the proofs are straightforward,
%% using inductive reasoning in the structure of
%% the data constructors and rewriting the definitions
%% of axiomatized functions.

\subsection{Functional Correctness} \label{subsec:programs}

Finally, we proved correctness of two programs
from the literature: a SAT solver and a Unification
algorithm.

\mypara{SAT Solver}
%
We implemented and verified the simple
SAT solver used to illustrate and evaluate
the features of the dependently typed language
Zombie~\citep{Zombie}.
%
The solver takes as input a formula @f@
and returns an assignment that
\emph{satisfies} @f@ if one exists.
%
\begin{code}
solve :: f:Formula -> Maybe {a:Asgn|sat a f}
solve f = find (`sat` f) (assignments f)
\end{code}
%
Function @assignments f@ returns all possible
assignments of the formula @f@ and @sat a f@
returns @True@ iff the assignment @a@ satisfies
the formula @f@:
%
\begin{code}
  reflect sat :: Asgn -> Formula -> Bool
  assignments :: Formula -> [Asgn]
\end{code}
%
Verification of @solve@ follows simply by
reflecting @sat@ into the refinement logic,
and using (bounded) refinements to show
that @find@ only returns values on which
its input predicate yields @True@~\cite{Vazou15}.
%
\begin{code}
  find :: p:(a -> Bool) -> [a]
       -> Maybe {v:a | p v}
\end{code}


\mypara{Unification}
%
As another example, we verified the
unification of first order terms, as
presented in~\citep{Sjoberg2015}.
%
First, we define a predicate alias for
when two terms @s@ and @t@ are equal
under a substitution @su@:
%
\begin{code}
  eq_sub su s t = apply su s == apply su t
\end{code}
%
Now, we can define a Haskell function
@unify s t@ that can diverge, or return
@Nothing@, or return a substitution @su@
that makes the terms equal:
%
\begin{code}
  unify :: s:Term -> t:Term
        -> Maybe {su| eq_sub su s t}
\end{code}
%
For the specification and verification
we only needed to reflect @apply@ and
not @unify@; thus we only had to verify
that the former terminates, and not the latter.
%
% not that @unify@ terminates, which is a
% complicate proof.

%%% HERE
%
As before, we prove correctness by invoking
separate helper lemmas.
%
For example to prove the post-condition
when unifying a variable @TVar i@ with
a term @t@ in which @i@ \emph{does not}
appear, we apply a lemma @not_in@:
%
\begin{mcode}
  unify (TVar i) t2
    | not (i Set_mem freeVars t2)
    = Just (const [(i, t2)] $\because$ not_in i t2)

\end{mcode}
%%
\ie if @i@ is not free in @t@,
the singleton substitution yields @t@:
%
\begin{code}
  not_in :: i:Int
         -> t:{Term | not (i Set_mem freeVars t)}
         -> {eq_sub [(i, t)] (TVar i) t}
\end{code}
%%
%% \NV{Emphasize how real world - diverging code co-exists with refinement reflection}
