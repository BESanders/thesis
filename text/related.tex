\chapter{Related Work}\label{chapter:related}

\section{Refinement Types in Practice}\label{sec:realword:related}

Next, we situate \toolname with 
existing Haskell verifiers.

\spara{Dependent Types} are the basis of many verifiers, 
or more generally, proof assistants.
%
Verification of haskell code is possible with
``full'' dependently typed systems like Coq~\cite{coq-book}, 
Agda~\cite{norell07}, Idris~\cite{Brady13}, Omega~\cite{Sheard06}, and
 {$\lambda_\rightarrow$}~\cite{LohMS10}.
 %
 While these systems are highly expressive,
their expressiveness comes at the cost of making logical validity checking undecidable
thus rendering verification cumbersome.	
 %
 Haskell itself can be considered a dependently-typed language,
 as type level computation is allowed via 
 Type Families~\cite{McBride02},
 Singleton Types\cite{Weirich12}, 
 Generalized Algebraic  Datatypes (GADTs)~\cite{JonesVWW06, SchrijversJSV09}, 
 and type-level functions~\cite{ChakravartyKJ05}.
 %
Again, 
verification in haskell itself turns out to be quite painful~\cite{LindleyM13}.

\spara{Refinement Types} are a form of dependent types where 
invariants are encoded via a combination of types and predicates
from a restricted \emph{SMT-decidable} 
logic~\cite{Rushby98,pfenningxi98,Dunfield07,GordonTOPLAS2011}. 
%
\toolname uses Liquid Types~\cite{LiquidPLDI09} 
that restrict the invariants even more
to allow type inference, a crucial feature of a usable type system.
%
Even though the language of refinements is restricted,
as we presented, the combination of
Abstract Refinements~\cite{vazou13} 
with sophisticated measure definitions 
allows specification and verification of a wide variety 
of program properties.

\spara{Static Contract Checkers} 
like ESCJava~\cite{ESCJava} are a classical way of verifying 
correctness through assertions and pre- and post-conditions. 
%
\cite{XuPOPL09} describes a static contract checker for 
Haskell that uses symbolic execution to unroll procedures
upto some fixed depth, yielding weaker ``bounded'' soundness
guarantees.
% 

Similarly, Zeno~\cite{ZENO} is an automatic Haskell 
prover that combines unrolling with heuristics for rewriting
and proof-search. 
%%Based on rewriting, it is sound but 
%%``Zeno might loop forever'' when faced with 
%%non-termination.
%
Finally, the Halo~\cite{halo} contract checker encodes 
Haskell programs into first-order logic by directly 
modeling the code's denotational semantics,
again, requiring heuristics for instantiating axioms 
describing functions' behavior.
%


\spara{Totality Checking}
is feasible by GHC itself, via an option flag that warns of any incomplete patterns.
%
Regrettably, GHC's warnings are local, \ie
GHC will raise a warning for @head@'s partial definition, 
but not for its caller, as the programmer would desire.
%%(2)~ and preservative:
%%a warning will be raised for any incomplete pattern
%%without an attempt to reason if it is reachable or not.
%
Catch~\cite{catch}, 
a fully automated tool that tracks incomplete patterns,
addresses the above issue
%
by computing functions' pre- and post-conditions.
Moreover, catch statically analyses the code 
to track reachable incomplete patterns.
%
\toolname allows more precise analysis than catch, 
thus, by assigning the appropriate
types to $\star$Error functions (\S~\ref{sec:totality}) 
it tracks reachable incomplete patters 
%we get catch analysis
as a side-effect of verification.

\spara{Termination Analysis}
is crucial for \toolname's soundness 
and is implemented in a technique inspired by~\cite{XiTerminationLICS01}, 
%
Various other authors have proposed techniques to verify termination of
recursive functions, either using the ``size-change
principle''~\cite{JonesB04,Sereni05}, or by annotating types with size indices
and verifying that the arguments of recursive calls have smaller
indices~\cite{HughesParetoSabry96,BartheTermination}.
%
To our knowledge, none of the above analyses have been empirically
evaluated on large and complex real-world libraries.

AProVE~\cite{Giesl11} implements a powerful, fully-automatic
termination analysis for Haskell based on term-rewriting.
%
Compared to AProVE,
encoding the termination proof via 
refinements provides advantages that are crucial in 
large, real-world code bases. 
Specifically, refinements
let us
%
(1) prove termination over a subset 
    (not all) of inputs; many functions (\eg @fac@) 
    terminate only on @Nat@ inputs and not all @Int@s,
%
(2) encode pre-conditions, 
    post-conditions, and auxiliary invariants that 
    are essential for proving termination, (\eg @qsort@),
%
(3) easily specify non-standard 
    decreasing metrics and prove termination, (\eg @range@).
%
In each case, the code could be (significantly) 
\emph{rewritten} to be amenable to AProVE but this defeats
the purpose of an automatic checker.
%


\section{Refinement Types for Haskell}\label{sec:refinedhaskell:related}

Next we situate our work with closely related lines of research.

\spara{Dependent Types} are the basis of many verifiers, 
or more generally, proof assistants.
%
In this setting arbitrary terms may appear inside types,
so to prevent logical inconsistencies, and enable
the checking of type equivalence, all terms must
terminate.
%
``Full'' dependently typed systems like Coq~\cite{coq-book}, 
Agda~\cite{norell07}, and Idris~\cite{Brady13} typically use 
\emph{structural} checks where recursion is allowed on 
sub-terms of ADTs to ensure that \emph{all} terms terminate.
%
We differ in that, since the refinement logic is
restricted, we do not require that all functions terminate,
and hence, we can prove properties of possibly diverging 
functions like @collatz@ as well as lazy functions like @repeat@.
%
Recent languages like Aura~\citep{AURA} and Zombie~\citep{Zombie}
allow general recursion, but constrain the logic to a terminating 
sublanguage, as we do, to avoid reasoning 
about divergence in the logic.
%
In contrast to us, the above systems crucially assume 
\emph{call-by-value} semantics to ensure that binders are bound
to values, \ie cannot diverge.





   Haskell itself can be used to \emph{fake} ``lightweight'' dependent 
   types~\citep{ChakravartyKJ05,JonesVWW06,Weirich12}.
   In this style, the invariants are expressed in 
   a restricted~\citep{Jia10} total 
   index language and relationships (\eg $x<y$ and $y<z$) 
   are combined (\eg $x<z$) by explicitly constructing
   a term denoting the consequent from terms 
   denoting the antecedents.
   %
   On the plus side this ``constructive'' approach
   ensures soundness. 
   It is impossible to witness inconsistencies, 
   as doing so triggers diverging computations.
   %
   However, it is not easy to use restricted indices
   with explicitly constructed relations to verify 
   complex properties~\citep{hasochism}.


\spara{Refinement Types} are a form of dependent types where 
invariants are encoded via a combination of types and predicates
from a restricted \emph{SMT-decidable} 
logic~\cite{Rushby98,pfenningxi98,Dunfield07,GordonTOPLAS2011}. 
%
The restriction makes it safe to support arbitrary recursion, 
which has hitherto never been a problem for refinement types.
%
However, we show that this is because all the above systems 
implicitly assume that all free variables are bound to values, 
which is only guaranteed under CBV and, as we have seen, leads 
to unsoundness under lazy evaluation.



\spara{Tracking Divergent Computations}
The notion of type stratification to track potentially 
diverging computations dates to at least~\citep{ConstableS87} 
which uses $\bar{\typ}$ to encode diverging terms, and types 
$\efix{}$ as $(\bar{\typ}\rightarrow\bar{\typ}) \rightarrow \bar{\typ}$).
%
More recently, \cite{Capretta05} tracks diverging 
computations within a \emph{partiality monad}.
%
Unlike the above, we use refinements to 
obtain terminating fixpoints (\etfix{}), which let us prove 
the vast majority (of sub-expressions) in real world libraries 
as non-diverging, avoiding the restructuring that would
be required by the partiality monad.

\spara{Termination Analyses}
Various authors have proposed techniques to verify termination 
of recursive functions, either using the ``size-change principle'' 
\cite{JonesB04,Sereni05}, or by annotating types with size indices 
and verifying that the arguments of recursive calls have smaller 
indices~\cite{HughesParetoSabry96,BartheTermination}.
%
Our use of refinements to encode terminating fixpoints is most 
closely related to~\cite{XiTerminationLICS01}, but this work 
also crucially assumes CBV semantics for soundness.

AProVE~\cite{Giesl11} implements a powerful, fully-automatic
termination analysis for Haskell based on term-rewriting.
%
While we could use an external analysis like AProVE,
we have found that encoding the termination proof via 
refinements provided advantages that are crucial in 
large, real-world code bases. Specifically, refinements
let us
%
(1) prove termination over a subset 
    (not all) of inputs; many functions (\eg @fac@) 
    terminate only on @Nat@ inputs and not all @Int@s,
%
(2) encode pre-conditions, 
    post-conditions, and auxiliary invariants that 
    are essential for proving termination, (\eg @gcd@),
%
(3) easily specify non-standard 
    decreasing metrics and prove termination, (\eg @range@).
%
In each case, the code could be (significantly) 
\emph{rewritten} to be amenable to AProVE but this defeats
the purpose of an automatic checker.
%
Finally, none of the above analyses have been empirically
evaluated on large and complex real-world libraries.


\spara{Static Contract Checkers} 
like ESCJava~\cite{ESCJava} are a classical way of verifying 
correctness through assertions and pre- and post-conditions. 
%
Side-effects like modifications of global variables are a 
well known issue for static checkers for imperative languages;
the standard approach is to use an effect analysis to determine
the ``modifies clause'' \ie the set of globals modified by a procedure.
%
Similarly, one can view our approach as implicitly computing 
the non-termination effects.
%
%
\cite{XuPOPL09} describes a static contract checker for 
Haskell that uses symbolic execution to unroll procedures
upto some fixed depth, yielding weaker ``bounded'' soundness
guarantees.
% 

%
Similarly, Zeno~\cite{ZENO} is an automatic Haskell 
prover that combines unrolling with heuristics for rewriting
and proof-search. 
Based on rewriting, it is sound but 
``Zeno might loop forever'' when faced with 
non-termination.
%
Finally, the Halo~\cite{halo} contract checker encodes 
Haskell programs into first-order logic by directly 
modeling the code's denotational semantics,
again, requiring heuristics for instantiating axioms 
describing functions' behavior. Halo's translation of Haskell
programs directly encodes constructors as uninterpreted functions,
axiomatized to be injective (as the denotational semantics requires).
This heavyweight encoding is more precise than predicate abstraction 
but leads to model-theoretic problems (outlined in the Halo paper) and 
affects the efficiency of the encoding when scaling to larger programs 
(see also \ref{sec:conclusion}, paragraph B) in the lack of specialized 
decisions procedures.
%
Unlike any of the above, our type-based approach does 
not rely on heuristics for unrolling recursive procedures, 
or instantiating axioms. 
%
Instead we are based on decidable SMT validity 
checking and abstract interpretation~\cite{LiquidPLDI08} 
which makes the tool predictable and the overall workflow
scale to the verification of large, real-world
code bases.

\section{Abstract Refinement Types}\label{sec:related}

The notion of type refinements was introduced by Freeman and
Pfenning~\cite{FreemanPfenning91}, with refinements limited to
restrictions on the structure of algebraic datatypes, for which
inference is decidable.
%
Our present notion of refinement types has its roots in the
\emph{indexed types} of Xi and Pfenning~\cite{pfenningxi98}, wherein
data types' ranges are restricted by \emph{indices}, analogous to our
refinement predicates, drawn from a decidable domain; in the example
case explored by Xi and Pfenning, types were indexed by terms from
Presburger arithmetic.
%
Since then, several approaches to developing richer refinement type
systems and accompanying methods for type checking have been
developed.
%
Knowles and Flanagan~\cite{Knowles10} allow refinement predicates to
be arbitrary terms of the language being typechecked and present a
technique for deciding some typing obligations statically and
deferring others to runtime.
%; Gronksi \etal~\cite{Gronski06} present animplementation of such a system.
%
Findler and Felleisen's~\cite{Findler02} higher-order contracts, which
extend Eiffel's~\cite{MeyerBook} first-order contracts --- ordinary
program predicates acting as dynamic pre- and post-conditions --- to
the setting of higher-order programs, eschew any form of static
checking, and can be seen as a dynamically-checked refinement type
system.
%
Bengtson \etal~\cite{GordonTOPLAS2011} present a refinement type
system in which type refinements are drawn from a decidable logic,
making static type checking tractable.
%
Greenberg \etal~\cite{Greenberg11} gives a rigorous treatment of the
metatheoretic properties of such a refinement type system.

Refinement types have been applied to the verification of a variety of
program properties~\cite{pfenningxi98,Dunfield,GordonTOPLAS2011,FournetCCS11}.
%
In the most closely related work to our own, Kawaguchi \etal~\cite{LiquidPLDI09} 
introduce \emph{recursive} and \emph{polymorphic} refinements for data
structure properties.
%
The present work unifies and generalizes these two somewhat ad-hoc notions 
into a single, strictly and significantly more expressive mechanism of
abstract refinements.

%  Higher-order logics: Coq/HTT/F*/Agda which have explicit predicates, quantification 
A number of higher-order logics and corresponding verification tools
have been developed for reasoning about programs.
%
Example of systems of this type include NuPRL \cite{Constable86},
%F$_{<:}$ \cite{Cardelli91},
Coq \cite{coq-book}, F$^\star$ \cite{SwamyCFSBY11} and Agda \cite{norell07}
which support the development and verification of higher-order, 
pure functional programs.
%
While these systems are highly expressive, their expressiveness comes at the
cost of making logical validity checking undecidable.
%
To help automate validity checking, both built-in and user-provided
tactics are used to attempt to discharge proof obligations; however,
the user is ultimately responsible for manually proving any
obligations which the tactics are unable to discharge.

\section{Bounded Refinement Types}\label{sec:abstractrefinements:related}

\paragraph{Higher order Logics and Dependent Type Systems}
%
including
NuPRL~\citep{Constable86},
Coq~\citep{coq-book}, Agda~\citep{norell07},
and even to some extent, \haskell~\citep{JonesVWW06, McBride02},
occupy the maximal extreme of the expressiveness spectrum.
However, in these settings, checking requires explicit
proof terms which can add considerable programmer overhead.
%
Our goal is to eliminate the programmer overhead of
proof construction by restricting specifications to
decidable, first order logics and to see how far
we can go without giving up on expressiveness.
%
The \fstar system enables full dependent typing via
SMT solvers via a higher-order universally quantified
logic that permit specifications similar to ours
(\eg @compose@, @filter@ and @foldr@).
%% https://github.com/FStarLang/FStar/
%
While this approach is at least as expressive
as bounded refinements it has two drawbacks.
%
First, due to the quantifiers, the generated VCs
fall outside the SMT decidable theories.
This renders the type system undecidable (in theory),
forcing a dependency on the solver's unpredictable
quantifier instantiation heuristics (in practice).
%
Second, more importantly, % perhaps more importantly,
the higher order
predicates must be \emph{explicitly} instantiated,
placing a heavy annotation burden on the programmer.
%
In contrast, bounds permit decidable
checking, and are automatically instantiated
via Liquid Types.


\paragraph{Our notion of Refinement Types}
%
has its roots in the predicate subtyping
of PVS~\cite{Rushby98} and \emph{indexed types}
(DML~\cite{pfenningxi98}) where types are constrained
by predicates drawn from a logic.
%
To ensure decidable checking several refinement
type systems including~\citep{pfenningxi98,Dunfield07,LiquidICFP14}
restrict refinements to decidable, quantifier free logics.
%
While this ensures predictable checking and inference~\cite{LiquidPLDI08}
it severely limits the language of specifications, and makes it hard to
fashion simple higher order abstractions like @filter@ (let alone the more
complex ones like relational algebras and state transformers.)

\paragraph{To Reconcile Expressiveness and Decidability}
%
\catalyst~\citep{catalyst} permits a form of
higher order specifications where refinements
are relations which may themselves be parameterized
by other relations, which allows for example, a
way to precisely type @filter@ by suitably
composing relations.
%
However, to ensure decidable checking, \catalyst
is limited to relations that can be specified as
catamorphisms over inductive types, precluding
for example, theories like arithmetic.
More importantly, (like \fstar), \catalyst provides
no inference: higher order relations must be
\emph{explicitly} instantiated.
%
Bounded refinements build directly upon
abstract refinements~\citep{vazou13},
a form of refinement polymorphism
analogous to parametric polymorphism.
%
While \cite{vazou13} adds expressiveness via
abstract refinements, without bounds we cannot
specify any \emph{relationships between} the
abstract refinements. The addition of bounds
makes it possible to specify and verify the examples
shown in this paper,
while preserving decidability and inference.

\paragraph{Our Relational Algebra Library} builds on a long
line of work on type safe database access.
%
The HaskellDB~\citep{haskellDB}
showed how phantom types could be used to eliminate
certain classes of errors.
%
Haskell's HList library~\citep{heterogeneous}
extends this work with type-level computation
features to encode heterogeneous lists, which
can be used to encode database schema, and
(unlike HaskellDB) statically reject accesses
of ``missing'' fields.
%
The HList implementation is non-trivial,
requiring new type-classes for new operations
(\eg @append@ing lists); \citep{thepipower}
shows how a dependently typed language greatly
simplifies the implementation.
%
Much of this simplicity can be recovered in
Haskell using the @singleton@ library~\citep{Weirich12}.
%
Our goal is to show that bounded refinements
are expressive enough to permit the construction
of rich abstractions like a relational algebra
and generic combinators for safe database access
while using SMT solvers to provide decidable
checking and inference. Further, unlike the
HList based approaches, refinements they can
be used to \emph{retroactively} or \emph{gradually}
verify safety; if we erase the types we still
get a valid Haskell program operating over
homogeneous lists.


\paragraph{Our Approach for Verifying Stateful Computations} using monads
indexed by pre- and post-conditions is inspired by the method of
Filli\^atre~\citep{Filliatre98}, which was later enriched with
separation logic in Ynot~\citep{ynot}. In future work it would
be interesting to use separation logic based refinements to specify
and verify the complex sharing and aliasing patterns allowed by Ynot.
%
\fstar encodes stateful computations in a special Dijkstra
Monad~\citep{dijkstramonad} that replaces the two assertions with
a single (weakest-precondition) predicate transformer which
can be composed across sub-computations to yield a transformer
for the entire computation.
%
Our \RIO approach uses the idea of indexed monads but
has two concrete advantages.
%
First, we show how bounded refinements alone suffice to
let us fashion the \RIO abstraction from scratch.
%
Consequently, second, we automate inference of pre- and
post-conditions and loop invariants as refinement instantiation
via Liquid Typing~\citep{LiquidPLDI08}.
