\section{Verified Parallelization of Monoid Morphisms}\label{sec:parallelization}

A monoid morphism is a function between two monoids which
preserves the monoidal structure; \ie a function on the underlying
sets which preserves identity and associativity. We formally specify
this definition using a refinement type @Morphism@.
%
\begin{definition}[Monoid Morphism]\label{definition:morphism}
A function @f :: n -> m@ is a morphism
between the monoids
(@m@, @$\epsilon$@, @<>@)
and (@n@, @$\eta$@, @<+>@),
if @Morphism n m f@ has an inhabitant.
\begin{code}
  type Morphism n m F 
    = x:n -> y:n -> {F eta = epsilon && F (x <+> y) = F x <> F y}
\end{code}
\end{definition}

A monoid morphism can be parallelized when its domain can be cut into
chunks and put back together again, a property we refer to as
chunkable and expand upon in \S~\ref{subsec:chunkable}. A
chunkable monoid morphism is then parallelized by:
\begin{itemize}
  \item chunking up the input,
  \item applying the morphism in parallel to all chunks, and
  \item recombining the chunks, also in parallel, back to a single value.
\end{itemize}
In the rest of this section we implement and verify to be correct the above
transformation.

\subsection{Chunkable Monoids}\label{subsec:chunkable}
\begin{definition}[Chunkable Monoids]\label{definition:chunkable}
A monoid (@m@, @epsilon@, @<>@) is chunkable
if the following four functions are defined on @m@.
\begin{code}
  length_m :: m -> Nat

  drop_m   :: i:Nat -> x:MGEq m i -> MEq m (length_m x - i)
  take_m   :: i:Nat -> x:MGEq m i -> MEq m i

  takeDropProp_m :: i:Nat -> x:m -> {x = take_m i x <> drop_m i x}
\end{code}

Where the type aliases @MLeq m I@ (and @MEq m I@)
constrain the monoid @m@ to have @length_m@
greater than (resp. equal) to @I@.
\begin{code}
  type MGEq m I = {x:m | I <= length_m x}
  type MEq  m I = {x:m | I =  length_m x}
\end{code}
\end{definition}

Note that the ``important'' methods of chunkable monoids
are the @take@ and @drop@, while the @length@ method is required
to give pre- and post-condition on the other operations.
%
Finally, @takeDropProp@ provides a proof that
for each @i@ and monoid @x@, appending
@take i x@ to @drop i x@ will reconstruct @x@.

Using @take_m@ and @drop_m@ we define for each chunkable monoid
(@m@, @epsilon@, @<>@) a function @chunk_m i x@ that
splits @x@ in chunks of size @i@.
\begin{code}
  chunk_m :: i:Pos -> x:m -> {v:L m | chunkRes_m i x v}
  chunk_m i x
    | length_m x <= i = C x N
    | otherwise      = take_m i x `C` chunk_m i (drop_m i x)

  chunkRes_m i x v
    | length_m x <= i = length_m v == 1
    | i == 1         = length_m v == length_m xs
    | otherwise      = length_m v <  length_m xs
\end{code}

%
The function @chunk_m@ provably terminates as
@drop_m i x@
will return a monoid smaller than @x@,
by the Definition of @drop_m@.
%
The definitions of both @take_m@ and @drop_m@
are also used from Liquid Haskell to verify the
@length_m@ constraints in the result of @chunk_m@.

\ignore{
As a concrete example, to define list chunking, we first define the @take@ and @drop@
methods on the list monoid of section~\ref{sec:haskell-proofs}.
%
\begin{code}
  take i N                    = N
  take i (C x xs) | i == 0    = N
                  | otherwise = C x (take (i-1) xs)

  drop i N                    = N
  drop i (C x xs) | i == 0    = C x xs
                  | otherwise = drop (i-1) xs
\end{code}
We can prove that the above definitions
combined with the @length@ of section~\ref{sec:haskell-proofs}
satisfy the specifications
of the Chunkable Monoid Definition~\ref{definition:chunkable}.
%
Thus, we can prove that the aforementioned list data type,
extended with the appropriate implementation for @takeDropProp@
is a chunkable monoid.
}

\subsection{Parallel Map}
We define a parallelized map function @pmap@
using Haskell's library @parallel@.
%
Concretely, we use the function
@Control.Parallel.Strategies.withStrategy@
that computes its argument in parallel given a parallel strategy.
\begin{code}
  pmap :: (a -> b) -> L a -> L b
  pmap f xs = withStrategy parStrategy (map f xs)
\end{code}
%
The strategy @parStrategy@ does not affect verification.
%
In our codebase we choose the traversable strategy.
\begin{code}
  parStrategy :: Strategy (L a)
  parStrategy = parTraversable rseq
\end{code}

\mypara{Parallelism in the Logic}
The function @withStrategy@ is an imported Haskell library function,
whose implementation is not available during verification.
%
To use it in our verified code, we make the \textit{assumption}
that it always returns its second argument.
\begin{code}
  assume withStrategy :: Strategy a -> x:a -> {v:a | v = x}
\end{code}
%
Moreover, we need to reflect the function @pmap@ and represent its
implementation in the logic.
%
Thus, we also need to represent the function @withStrategy@ in the logic.
%
LiquidHaskell represents @withStrategy@ in the logic as a logical
function that merely returns
its second argument, @withStrategy _ x = x@,
and does not reason about parallelism.


\subsection{Monoidal Concatenation}\label{subsec:mconcat}
The function @chunk_m@ allows chunking a monoidal value into several
pieces. Dually, for any monoid @m@, there is a
standard way of turning @L m@ back into a single @m@~\footnote{\texttt{mconcat} is usually defined as \texttt{foldr mappend mempty}}.
\begin{code}
  mconcat :: L m -> m
  mconcat N        = mempty
  mconcat (C x xs) = x <> mconcat xs
\end{code}
%
For any chunkable monoid @n@,
%
monoid morphism @f :: n -> m@,
%
and natural number @i > 0@
%
we can write a chunked version of @f@ as
\begin{code}
  mconcat . pmap f . chunk_n i :: n -> m.
\end{code}
Before parallelizing @mconcat@, we will prove that the previous function is equivalent to @f@.

\begin{theorem}[Morphism Distribution]\label{theorem:monoid:distribution}
Let (@m@, @$\epsilon$@, @<>@) be a monoid
and (@n@, @$\eta$@, @<+>@) be a chunkable monoid.
%
Then, for every morphism @f :: n -> m@,
every positive number @i@ and input @x@,
@f x = mconcat (pmap f (chunk_n i x))@ holds.
%
\begin{code}
  morphismDistribution
    :: f:(n -> m) -> Morphism n m f -> x:n -> i:Pos
    -> {f x = mconcat (pmap f (chunk_n i x))}
\end{code}
\end{theorem}

\begin{proof}
We prove the theorem by implementing
@morphismDistribution@ in a way that satisfies its type.
%
The proof proceeds by induction on the length of the input.
%
\begin{code}
  morphismDistribution f thm x i
    | length_n x <= i
    =  mconcat (pmap f (chunk_n i x))
    =. mconcat (map f (chunk_n i x))
    =. mconcat (map f (C x N))
    =. mconcat (f x `C` map f N)
    =. f is <> mconcat N
    =. f is <> epsilon
    =. f is ? idRight_m (f is)
    ** QED
    
  morphismDistribution f thm x i
    =   mconcat (pmap f (chunk_n i x))
    =. mconcat (map f (chunk_n i x))
    =. mconcat (map f (C takeX) (chunk_n i dropX)))
    =. mconcat (f takeX `C` map f (chunk_n n dropX))
    =. f takeX <> f dropX   ? morphismDistribution f thm dropX i
    =. f (takeX <+> dropX)   ? thm takeX dropX
    =. f x                 ? takeDropProp_n i x
    ** QED
    where
      dropX = drop_n i x
      takeX = take_n i x
\end{code}
%
In the base case we use rewriting and right identity on the monoid @f x@.
%
In the inductive case,
we use the inductive hypothesis on the input @dropX = drop_n i x@,
that is provably smaller than @x@ as @1 < i@.
%
Then, the fact that @f@ is a monoid morphism,
as encoded by our assumption argument @thm takeX dropX@
we get basic distribution of @f@, that is
@f takeX <> f dropX = f (takeX <+> dropX)@.
%
Finally, we merge @takeX <+> dropX@ to @x@
using the property @takeDropProp_n@ of the chunkable monoid @n@.
\cqed\end{proof}


\subsection{Parallel Monoidal Concatenation}\label{subsec:pmconcat}
%
We now parallelize the monoid concatenation by defining a
@pmconat i x@ function that chunks the input list of monoids and concatenates each
chunk in parallel.

We use the @chunk@ function of \S~\ref{subsec:chunkable} instantiated to @L m@ to define a parallelized version of
monoid concatenation @pmconcat@.
\begin{code}
  pmconcat :: Int -> L m -> m
  pmconcat i x | i <= 1 || length x <= i
    = mconcat x
  pmconcat i x
    = pmconcat i (pmap mconcat (chunk i x))
\end{code}
The function @pmconcat i x@ calls @mconcat x@ in the base case,
otherwise it
(1) chunks the list @x@ in lists of size @i@,
(2) runs in parallel @mconcat@ to each chunk,
(3) recursively runs itself with the resulting list.
%
Termination of @pmconcat@ holds, as the length of @chunk i x@
is smaller than the length of @x@, when @1 < i@.

Next, we prove equivalence of parallelized monoid concatenation.
%
\begin{theorem}[Correctness of Parallelization]\label{theorem:equivalence:concat}
Let (@m@, @$\epsilon$@, @<>@) be a monoid.
Then, the parallel and sequential concatenations are equivalent.
\begin{code}
  pmconcatEq :: i:Int -> x:L m -> {pmconcat i x = mconcat x}
\end{code}
\end{theorem}

\begin{proof}
We prove the theorem by providing a Haskell implementation of @pmconcatEq@
that satisfies its type.
%
The details of the proof can be found in~\cite{implementation},
here we provide the sketch of the proof.

First, we prove that @mconcat@ distributes over list splitting
\begin{code}
  mconcatSplit
    :: i:Nat -> xs:{L m | i <= length xs}
    -> {mconcat xs = mconcat (take i xs) <> mconcat (drop i xs)}
\end{code}
%
The proofs proceeds by structural induction, using monoid left identity in the base case
and monoid associativity associavity and unfolding of @take@ and @drop@
methods in the inductive step.

We generalize the above
to prove that @mconcat@ distributes over list chunking.
\begin{code}
  mconcatChunk
    :: i:Pos -> xs:L m
    -> {mconcat xs = mconcat (map mconcat (chunk i xs))}
\end{code}
%
The proofs proceeds by structural induction, using monoid left identity in the base case
and lemma @mconcatSplit@ in the inductive step.

Lemma @mconcatChunk@ is sufficient to prove @pmconcatEq@ by structural induction,
using monoid left identity in the base case.
\cqed\end{proof}

\subsection{Parallel Monoid Morphism}\label{subsec:both-levels}
We can now replace the @mconcat@ in our chunked monoid morphism in
\S~\ref{subsec:mconcat} with @pmconcat@ from
\S~\ref{subsec:pmconcat} to provide an implementation that uses
parallelism to both map the monoid morphism and concatenate the
results.

%\paragraph{Correctness} of our parallel monoid morphism follows from Theorems~\ref{theorem:monoid:distribution} and~\ref{theorem:equivalence:concat}.
%
\begin{theorem}[Correctness of Parallelization]\label{theorem:two-level}
Let (@m@, @$\epsilon$@, @<>@) be a monoid
and (@n@, @$\eta$@, @<+>@) be a chunkable monoid.
%
Then, for every morphism @f :: n -> m@,
every positive numbers @i@ and @j@, and input @x@,
@f x = pmconcat i (pmap f (chunk_n j x))@ holds.
%
\begin{code}
  parallelismEq
    :: f:(n -> m) -> Morphism n m f -> x:n -> i:Pos -> j:Pos
    -> {f x = pmconcat i (pmap f (chunk_n j x))}
\end{code}
\end{theorem}

\begin{proof}
We prove the theorem by providing an implementation of
@parallelismEq@ that satisfies its type.
%
\begin{code}
  parallelismEq f thm x i j
    =   pmconcat i (pmap f (chunk_n j x))
    =. mconcat (pmap f (chunk_n j x))
       ? pmconcatEq i (pmap f (chunk_n j x))
    =. f x
       ? morphismDistribution f thm x j
    ** QED
\end{code}
The proof follows merely by application of the 
two previous Theorems~\ref{theorem:monoid:distribution} and~\ref{theorem:equivalence:concat}.
\cqed\end{proof}


\mypara{A Basic Time Complexity} analysis of the algorithm
reveals that parallelization of morphism  leads to runtime speedups
on monads with fast (constant time) appending operator.

We want to compare the complexities of the sequential @f i@
and the two-level parallel @pmconcat i (pmap f (chunk_n j x))@.
%
Let $n$ be the size on the input @x@.
Then, the sequential version runs in time
$T_f(n) = O(n)$, that is equal to the time complexity of the morphism @f@ on input @i@.
%

The parallel version runs @f@ on inputs of size $n' = \frac{n}{j}$.
%
Assuming the complexity of @x <> y@ to be $T_\mappend(\text{max}(|\tx|, |\ty|))$,
complexity of @mconcat xs@ is $O((\texttt{length \txs}-1) T_\mappend(\text{max}_{\tx_i \in \txs}(|\tx_i|)))$.
%
Now, parallel concatenation, @pmconcat i xs@ at each iteration runs @mappend@
on a list of size @i@. Moreover,
at each iteration, divides the input list in chunks of size @i@, leading to
$\frac{\log|xs|}{\log i}$ iterations, and time complexity
$(i-1)(\frac{\log|xs|}{\log i})(T_\mappend(m))$
for some $m$ that bounds the size of the monoids.

The time complexity of parallel algorithm consists on the base cost on running @f@
at each chunk and then parallel concatenating the $\frac{n}{j}$ chunks.
\begin{equation}
O((i-1)(\frac{\log n - \log j}{\log i})T_\mappend(m) + T_f(\frac{n}{j})) \label{eq:complexity}
\end{equation}
%
Since time complexity depends on the time complexity of @<>@
for the parallel algorithm to be efficient time complexity of @<>@ should be constant.
%
Otherwise, if it depends on the size of the input, the size of monoids can grow at each iteration of @mconcat@.
%

Moreover, from the complexity analysis we observe that time grows on bigger @i@ and smaller @j@.
%
Thus, chunking the input in small chunks while splitting the monoid list in half leads
to more parallelism, and thus (assuming infinite processors and no caching) greatest speedup.
